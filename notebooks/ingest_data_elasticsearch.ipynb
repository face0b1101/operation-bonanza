{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell to avoid a known issue with some IDEs: https://github.com/astral-sh/ruff-vscode/issues/593"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Setting up Elasticsearch\n",
    "\n",
    "The process we'll follow is:\n",
    "\n",
    "1. Create an Elasticsearch client\n",
    "2. Check if the required ingest pipeline exists, and create it if not\n",
    "3. Check if the index with proper mappings exists, and create it if not\n",
    "4. Load the data into the index\n",
    "\n",
    "This ensures we don't overwrite existing settings if the index is already properly configured - and, importantly, don't ingest duplicate data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from collections.abc import Iterator\n",
    "from datetime import UTC, datetime, timezone\n",
    "\n",
    "from decouple import config\n",
    "from tqdm.auto import tqdm  # Use tqdm.auto for notebook-friendly bars\n",
    "\n",
    "from elasticsearch import Elasticsearch, NotFoundError\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# Load environment variables from .env file\n",
    "ES_ENDPOINT = config(\"ES_ENDPOINT\", default=\"\")\n",
    "ES_API_KEY = config(\"ES_API_KEY\", default=\"\")\n",
    "\n",
    "# define Elasticsearch config files\n",
    "es_index_name = \"data.service-legislation-ukpga\"\n",
    "es_index_settings_file = \"../elasticsearch/indices/data.service-legislation-ukpga.json\"\n",
    "\n",
    "es_ingest_pipeline_name = \"data.service-legislation-ukpga-pipeline\"\n",
    "es_ingest_pipeline_file = (\n",
    "    \"../elasticsearch/pipelines/data.service-legislation-ukpga-pipeline.json\"\n",
    ")\n",
    "\n",
    "model_id = \"i-dot-ai__all-minilm-l6-v2-ukpga-6k-finetune\"\n",
    "inference_id = \"iai-ukpga\"\n",
    "\n",
    "# define data directory\n",
    "data_dir = \"../data/source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Elasticsearch client\n",
    "if not ES_ENDPOINT or not ES_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"ES_ENDPOINT and ES_API_KEY must be set in the environment variables.\"\n",
    "    )\n",
    "\n",
    "# Initialize the client\n",
    "es_client = Elasticsearch(\n",
    "    hosts=[ES_ENDPOINT],\n",
    "    api_key=ES_API_KEY,\n",
    ")\n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ingest_pipeline(\n",
    "    es_client: Elasticsearch, pipeline_file: str, pipeline_name: str\n",
    ") -> bool:\n",
    "    \"\"\"Create the ingest pipeline if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        es_client: Elasticsearch client\n",
    "        pipeline_file: Path to the pipeline definition file\n",
    "        pipeline_name: Name of the pipeline to create\n",
    "\n",
    "    Returns:\n",
    "        bool: True if pipeline was created, False if it already existed\n",
    "    \"\"\"\n",
    "    # Check if pipeline exists\n",
    "    try:\n",
    "        es_client.ingest.get_pipeline(id=pipeline_name)\n",
    "        print(f\"Pipeline '{pipeline_name}' already exists\")\n",
    "        return False\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Pipeline '{pipeline_name}' not found, creating it...\")\n",
    "\n",
    "        # Load pipeline definition from file\n",
    "        with open(pipeline_file) as file:\n",
    "            pipeline_definition = json.load(file)\n",
    "\n",
    "        # Create the pipeline\n",
    "        es_client.ingest.put_pipeline(id=pipeline_name, body=pipeline_definition)\n",
    "\n",
    "        print(f\"Pipeline '{pipeline_name}' created successfully\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(es_client: Elasticsearch, index_file: str, index_name: str):\n",
    "    \"\"\"Create the index if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        es_client: Elasticsearch client\n",
    "        index_file: Path to the index definition file\n",
    "        index_name: Name of the index to create\n",
    "\n",
    "    Returns:\n",
    "        bool: True if index was created, False if it already existed\n",
    "    \"\"\"\n",
    "    # Check if index exists and store the result\n",
    "    if es_client.indices.exists(index=index_name).body:\n",
    "        print(f\"Index '{index_name}' already exists\")\n",
    "    else:\n",
    "        # If index does not exist, create it\n",
    "        print(f\"Index '{index_name}' not found, creating it with proper mappings...\")\n",
    "\n",
    "        # Load index definition from file\n",
    "        with open(index_file) as file:\n",
    "            index_definition = json.load(file)\n",
    "\n",
    "        # Create the index with settings and mappings\n",
    "        if es_client.indices.create(index=index_name, body=index_definition).body:\n",
    "            print(f\"Index '{index_name}' created successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to create index '{index_name}'\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_es_actions(filepath: str, index_name: str) -> Iterator[dict]:\n",
    "    \"\"\"Reads documents from a file and yields Elasticsearch bulk actions.\n",
    "\n",
    "    This function efficiently processes both standard JSON files (containing a\n",
    "    single list of objects) and JSON Lines (.jsonl) files (containing one\n",
    "    JSON object per line). It reads the file and yields data formatted as\n",
    "    an action dictionary, ready for use with the `elasticsearch.helpers.bulk` API.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the input file (.json or .jsonl).\n",
    "        index_name (str): The name of the Elasticsearch index to target.\n",
    "\n",
    "    Yields:\n",
    "        dict: An Elasticsearch bulk action dictionary in the format:\n",
    "              {\"_op_type\": \"index\", \"_index\": index_name, \"_source\": doc}.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    def yield_actions_from_docs(docs_iterator):\n",
    "        \"\"\"Inner helper to format documents from any iterator.\"\"\"\n",
    "        for doc in docs_iterator:\n",
    "            yield {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": doc,\n",
    "            }\n",
    "\n",
    "    # Handle JSONL files\n",
    "    if filepath.endswith(\".jsonl\"):\n",
    "        try:\n",
    "            with open(filepath, encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        doc = json.loads(line)\n",
    "                        yield {\n",
    "                            \"_op_type\": \"index\",\n",
    "                            \"_index\": index_name,\n",
    "                            \"_source\": doc,\n",
    "                        }\n",
    "\n",
    "        except (OSError, json.JSONDecodeError) as e:\n",
    "            print(f\"Skipping malformed or unreadable line/file {filename}: {e}\")\n",
    "\n",
    "    # Handle standard JSON files\n",
    "    elif filepath.endswith(\".json\"):\n",
    "        try:\n",
    "            with open(filepath, encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    # yields actions for each document in the list\n",
    "                    yield from yield_actions_from_docs(data)\n",
    "                else:\n",
    "                    print(f\"Skipping {filename} as it does not contain a JSON list.\")\n",
    "\n",
    "        except (OSError, json.JSONDecodeError) as e:\n",
    "            print(f\"Skipping malformed or unreadable file {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_elasticsearch(\n",
    "    es_client: Elasticsearch,\n",
    "    es_index_name: str,\n",
    "    data_dir: str,\n",
    "    chunk_size: int = 500,\n",
    "    request_timeout: int = 60,\n",
    "):\n",
    "    \"\"\"Finds and loads data from JSON/JSONL files into Elasticsearch using the bulk API.\n",
    "\n",
    "    This function scans a directory for .json and .jsonl files, reads them\n",
    "    efficiently using a generator, and ingests them into the specified\n",
    "    Elasticsearch index in batches for optimal performance.\n",
    "\n",
    "    During ingestion, any per-document errors reported by the bulk API are\n",
    "    written to log files under the ``notebooks/logs`` directory so they can\n",
    "    be inspected and debugged later.\n",
    "\n",
    "    Args:\n",
    "        es_client: An initialized Elasticsearch client instance.\n",
    "        es_index_name: The name of the index where data will be loaded.\n",
    "        data_dir: The path to the directory containing the data files.\n",
    "        chunk_size (int): The number of documents to send in each bulk request.\n",
    "        request_timeout (int): The timeout in seconds for each bulk request.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(os.path.join(data_dir, \"*.json\")) + glob.glob(\n",
    "        os.path.join(data_dir, \"*.jsonl\")\n",
    "    )\n",
    "\n",
    "    if not all_files:\n",
    "        print(f\"No .json or .jsonl files found in '{data_dir}'\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(all_files)} files to process...\")\n",
    "    total_successes = 0\n",
    "\n",
    "    # Prepare logs directory (relative to the notebook folder).\n",
    "    logs_dir = os.path.join(\"logs\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    # Create a client with timeout options\n",
    "    client_with_timeout = es_client.options(request_timeout=request_timeout)\n",
    "\n",
    "    # show progress for file processing\n",
    "    for filepath in tqdm(all_files, desc=\"Processing Files\"):\n",
    "        filename = os.path.basename(filepath)\n",
    "        error_log_path = os.path.join(\n",
    "            logs_dir,\n",
    "            f\"{filename}.errors.{datetime.now(UTC).strftime('%Y%m%dT%H%M%SZ')}.log\",\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Create the generator that yields ES bulk actions\n",
    "            action_generator = generate_es_actions(filepath, es_index_name)\n",
    "\n",
    "            # Use the bulk helper for efficient ingestion\n",
    "            success_count, errors = bulk(\n",
    "                client=client_with_timeout,\n",
    "                actions=action_generator,\n",
    "                chunk_size=chunk_size,\n",
    "                raise_on_error=False,  # Report errors instead of stopping\n",
    "            )\n",
    "            total_successes += success_count\n",
    "\n",
    "            if errors:\n",
    "                # Write detailed error information to a per-file log so we can\n",
    "                # inspect the root causes in notebooks/logs.\n",
    "                with open(error_log_path, \"w\", encoding=\"utf-8\") as log_f:\n",
    "                    log_f.write(\n",
    "                        f\"Ingestion errors for file '{filename}' into index '{es_index_name}'\\n\"\n",
    "                    )\n",
    "                    for err in errors:\n",
    "                        log_f.write(json.dumps(err, ensure_ascii=False))\n",
    "                        log_f.write(\"\\n\")\n",
    "\n",
    "                tqdm.write(\n",
    "                    f\"⚠️  Finished {filename}: Loaded {success_count} documents with {len(errors)} errors. \"\n",
    "                    f\"Details logged to '{error_log_path}'.\"\n",
    "                )\n",
    "            else:\n",
    "                tqdm.write(\n",
    "                    f\"✅ Finished {filename}: Loaded {success_count} documents successfully.\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log unexpected critical errors for this file as well.\n",
    "            with open(error_log_path, \"a\", encoding=\"utf-8\") as log_f:\n",
    "                log_f.write(\n",
    "                    f\"Critical error while ingesting '{filename}' into '{es_index_name}': {e}\\n\"\n",
    "                )\n",
    "            tqdm.write(\n",
    "                f\"❌ A critical error occurred with {filename}: {e}. \"\n",
    "                f\"See '{error_log_path}' for details.\"\n",
    "            )\n",
    "\n",
    "    print(\"\\n--- Ingestion Complete ---\")\n",
    "    print(f\"Total documents loaded successfully from all files: {total_successes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_ml_model(\n",
    "    es_client: Elasticsearch,\n",
    "    model_id: str = model_id,\n",
    "    priority: str = \"normal\",\n",
    "    hf_model_id: str | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"Ensure a trained model exists (uploading from Hugging Face if needed) and deploy it.\n",
    "\n",
    "    This function focuses purely on the ML model lifecycle:\n",
    "    1. Verifies that the trained model is present in the cluster\n",
    "    2. If it is missing, imports it from Hugging Face using Eland\n",
    "    3. Starts or updates the model deployment with the specified priority\n",
    "\n",
    "    Args:\n",
    "        es_client: Elasticsearch client instance\n",
    "        model_id: The trained model ID to check and deploy (with underscores, not slashes)\n",
    "        priority: Deployment priority ('low', 'normal', or 'high')\n",
    "        hf_model_id: Optional Hugging Face model identifier to import if the model\n",
    "            is not yet present in the cluster. If omitted, we fall back to a simple\n",
    "            mapping of ``model_id.replace(\"__\", \"/\")``.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - model_exists (bool): Whether the model is now present in the cluster\n",
    "            - deployment_status (str): Status of the model deployment\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model does not exist and cannot be imported\n",
    "        Exception: For other Elasticsearch API errors\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"model_exists\": False,\n",
    "        \"deployment_status\": \"not_checked\",\n",
    "    }\n",
    "\n",
    "    # Step 1: Check if the trained model exists, importing from Hugging Face if needed.\n",
    "    try:\n",
    "        print(f\"Checking for trained model: {model_id}\")\n",
    "        models_response = es_client.ml.get_trained_models(model_id=model_id)\n",
    "\n",
    "        if models_response.body.get(\"count\", 0) > 0:\n",
    "            result[\"model_exists\"] = True\n",
    "            print(f\"✓ Model '{model_id}' found in cluster\")\n",
    "        else:\n",
    "            # The API responded but returned no models; treat as not found.\n",
    "            raise NotFoundError(\"no_trained_models\", \"Model not found in cluster\")\n",
    "\n",
    "    except NotFoundError:\n",
    "        # The client raises NotFoundError when the model id is unknown. In this case\n",
    "        # we pivot to importing the model from Hugging Face using Eland.\n",
    "        effective_hf_model_id = hf_model_id or model_id.replace(\"__\", \"/\")\n",
    "        print(\n",
    "            f\"Model '{model_id}' not found in cluster. Attempting import from Hugging Face \"\n",
    "            f\"as '{effective_hf_model_id}'...\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            from pathlib import Path\n",
    "\n",
    "            from eland.ml.pytorch import PyTorchModel\n",
    "            from eland.ml.pytorch.transformers import TransformerModel\n",
    "        except ImportError as import_err:  # pragma: no cover - environment-specific\n",
    "            raise ImportError(\n",
    "                \"The 'eland' package is required to import models from Hugging Face. \"\n",
    "                \"Install it in this environment, for example with 'uv add eland' or \"\n",
    "                \"'pip install eland', and re-run this cell.\"\n",
    "            ) from import_err\n",
    "\n",
    "        try:\n",
    "            # Download and export the Hugging Face model via Eland. The\n",
    "            # TransformerModel API expects keyword-only arguments, so we pass the\n",
    "            # Hugging Face identifier as `model_id` rather than a positional arg.\n",
    "            transformer_model = TransformerModel(\n",
    "                model_id=effective_hf_model_id,\n",
    "                task_type=\"text_embedding\",\n",
    "            )\n",
    "\n",
    "            models_dir = Path(\"models\")\n",
    "            models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            model_path, config, vocab_path = transformer_model.save(models_dir)\n",
    "\n",
    "            # Import the TorchScript model into Elasticsearch as a trained model.\n",
    "            ptm = PyTorchModel(es_client, model_id)\n",
    "            ptm.import_model(\n",
    "                model_path=model_path,\n",
    "                config_path=None,\n",
    "                vocab_path=vocab_path,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "            result[\"model_exists\"] = True\n",
    "            print(\n",
    "                f\"✓ Hugging Face model '{effective_hf_model_id}' imported into Elasticsearch \"\n",
    "                f\"as trained model '{model_id}'\"\n",
    "            )\n",
    "\n",
    "        except Exception as import_error:\n",
    "            raise ValueError(\n",
    "                f\"Failed to import Hugging Face model '{effective_hf_model_id}' into \"\n",
    "                f\"Elasticsearch as '{model_id}': {import_error}\"\n",
    "            ) from import_error\n",
    "\n",
    "    except Exception:\n",
    "        # Bubble up unexpected errors during model existence/import checks.\n",
    "        raise\n",
    "\n",
    "    # Step 2: Start or update model deployment with the requested priority\n",
    "    try:\n",
    "        print(\n",
    "            f\"Starting/updating deployment for model: {model_id} with {priority} priority\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            deployment_stats = es_client.ml.get_trained_models_stats(\n",
    "                model_id=model_id\n",
    "            ).body\n",
    "            current_deployment = None\n",
    "\n",
    "            if deployment_stats.get(\"count\", 0) > 0:\n",
    "                trained_models = deployment_stats.get(\"trained_model_stats\", [])\n",
    "                if trained_models and \"deployment_stats\" in trained_models[0]:\n",
    "                    current_deployment = trained_models[0][\"deployment_stats\"]\n",
    "\n",
    "            # If already deployed, check if we need to update priority\n",
    "            if current_deployment:\n",
    "                current_state = current_deployment.get(\"state\", \"\")\n",
    "                print(f\"  Current deployment state: {current_state}\")\n",
    "\n",
    "                if current_state == \"started\":\n",
    "                    result[\"deployment_status\"] = \"already_started\"\n",
    "                    print(\"  Model deployment already active\")\n",
    "                else:\n",
    "                    es_client.ml.start_trained_model_deployment(\n",
    "                        model_id=model_id,\n",
    "                        priority=priority,\n",
    "                        wait_for=\"started\",\n",
    "                    )\n",
    "                    result[\"deployment_status\"] = \"started\"\n",
    "                    print(f\"✓ Model deployment started with {priority} priority\")\n",
    "            else:\n",
    "                es_client.ml.start_trained_model_deployment(\n",
    "                    model_id=model_id,\n",
    "                    priority=priority,\n",
    "                    wait_for=\"started\",\n",
    "                )\n",
    "                result[\"deployment_status\"] = \"started\"\n",
    "                print(f\"✓ Model deployment started with {priority} priority\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"resource_already_exists_exception\" in str(e):\n",
    "                result[\"deployment_status\"] = \"already_started\"\n",
    "                print(\"  Model deployment already exists and is active\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error starting model deployment: {e}\")\n",
    "        result[\"deployment_status\"] = f\"error: {e}\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_endpoint(\n",
    "    es_client: Elasticsearch,\n",
    "    model_id: str = model_id,\n",
    "    inference_id: str = inference_id,\n",
    "    request_timeout: int = 30,\n",
    ") -> dict:\n",
    "    \"\"\"Create a text-embedding inference endpoint for the given model.\n",
    "\n",
    "    This version reads the current model deployment stats and mirrors the\n",
    "    number of allocations and threads per allocation so the inference\n",
    "    endpoint cannot request more resources than the deployment provides.\n",
    "\n",
    "    Args:\n",
    "        es_client: Elasticsearch client instance\n",
    "        model_id: The trained model ID that the endpoint should use\n",
    "        inference_id: The ID to use for the inference endpoint\n",
    "        request_timeout: HTTP request timeout in seconds for the client call\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - inference_created (bool): Whether the create request was sent\n",
    "            - inference_status (str): Status message for inference endpoint\n",
    "\n",
    "    Raises:\n",
    "        Exception: For Elasticsearch API errors\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"inference_created\": False,\n",
    "        \"inference_status\": \"not_attempted\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"Creating inference endpoint: {inference_id}\")\n",
    "\n",
    "        # Check if inference endpoint already exists\n",
    "        try:\n",
    "            existing_inference = es_client.inference.get(inference_id=inference_id)\n",
    "            if existing_inference:\n",
    "                result[\"inference_created\"] = False\n",
    "                result[\"inference_status\"] = \"already_exists\"\n",
    "                print(f\"  Inference endpoint '{inference_id}' already exists\")\n",
    "                return result\n",
    "        except NotFoundError:\n",
    "            # Endpoint does not exist yet, proceed to create it\n",
    "            pass\n",
    "        except Exception:\n",
    "            # Surface unexpected errors\n",
    "            raise\n",
    "\n",
    "        # Build service_settings based on the current model deployment so we\n",
    "        # never exceed the allocations/threads that the deployment has.\n",
    "        service_settings: dict[str, object] = {\"model_id\": model_id}\n",
    "\n",
    "        try:\n",
    "            stats = es_client.ml.get_trained_models_stats(model_id=model_id).body\n",
    "            trained_models = stats.get(\"trained_model_stats\", [])\n",
    "            deployment_stats = None\n",
    "            if trained_models and \"deployment_stats\" in trained_models[0]:\n",
    "                deployment_stats = trained_models[0][\"deployment_stats\"]\n",
    "\n",
    "            if deployment_stats:\n",
    "                num_allocations = deployment_stats.get(\"number_of_allocations\")\n",
    "                num_threads = deployment_stats.get(\"threads_per_allocation\")\n",
    "\n",
    "                if isinstance(num_allocations, int) and num_allocations > 0:\n",
    "                    service_settings[\"num_allocations\"] = num_allocations\n",
    "                if isinstance(num_threads, int) and num_threads > 0:\n",
    "                    service_settings[\"num_threads\"] = num_threads\n",
    "        except Exception as stats_error:\n",
    "            # If we cannot read deployment stats, fall back to a conservative\n",
    "            # single-allocation, single-thread configuration.\n",
    "            print(\n",
    "                f\"⚠️  Could not read deployment stats for '{model_id}', \"\n",
    "                f\"using defaults: {stats_error}\"\n",
    "            )\n",
    "            service_settings.setdefault(\"num_allocations\", 1)\n",
    "            service_settings.setdefault(\"num_threads\", 1)\n",
    "\n",
    "        inference_config = {\n",
    "            \"service\": \"elasticsearch\",\n",
    "            \"service_settings\": service_settings,\n",
    "        }\n",
    "\n",
    "        # Initiate inference endpoint creation asynchronously. By setting the API\n",
    "        # `timeout` parameter to \"0s\" we ask Elasticsearch to return immediately\n",
    "        # after the request is accepted, instead of waiting for the endpoint to be\n",
    "        # fully created. You can monitor progress separately via the ML stats APIs\n",
    "        # or the UI.\n",
    "        print(\"  Initiating inference endpoint creation (async, timeout=0s)...\")\n",
    "        client_with_timeout = es_client.options(request_timeout=request_timeout)\n",
    "        client_with_timeout.inference.put(\n",
    "            inference_id=inference_id,\n",
    "            task_type=\"text_embedding\",\n",
    "            inference_config=inference_config,\n",
    "            timeout=f\"{request_timeout}s\",\n",
    "        )\n",
    "\n",
    "        result[\"inference_created\"] = True\n",
    "        result[\"inference_status\"] = \"creation_requested\"\n",
    "        print(\n",
    "            f\"✓ Inference endpoint creation request for '{inference_id}' sent (not waiting for full creation)\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"resource_already_exists_exception\" in str(e):\n",
    "            result[\"inference_created\"] = False\n",
    "            result[\"inference_status\"] = \"already_exists\"\n",
    "            print(f\"  Inference endpoint '{inference_id}' already exists\")\n",
    "        else:\n",
    "            print(f\"⚠️  Error creating inference endpoint: {e}\")\n",
    "            result[\"inference_status\"] = f\"error: {e}\"\n",
    "            raise\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_inference_endpoint(\n",
    "    es_client: Elasticsearch,\n",
    "    model_id: str = model_id,\n",
    "    inference_id: str = inference_id,\n",
    "    priority: str = \"normal\",\n",
    "    inference_timeout: int = 600,\n",
    ") -> dict:\n",
    "    \"\"\"Convenience wrapper that deploys the model then creates an inference endpoint.\n",
    "\n",
    "    This function preserves the original notebook behaviour while delegating to\n",
    "    two smaller helpers:\n",
    "\n",
    "    1. ``deploy_ml_model`` ensure the trained model exists and is deployed\n",
    "    2. ``create_inference_endpoint`` request creation of a text-embedding\n",
    "       inference endpoint for that model (asynchronously).\n",
    "\n",
    "    Args:\n",
    "        es_client: Elasticsearch client instance\n",
    "        model_id: The trained model ID to check and deploy (with underscores, not slashes)\n",
    "        inference_id: The ID to use for the inference endpoint\n",
    "        priority: Deployment priority ('low', 'normal', or 'high')\n",
    "        inference_timeout: Used here as the HTTP request timeout for the endpoint\n",
    "            creation call. Since we use ``timeout=\"0s\"`` at the API level, this is\n",
    "            mainly a safety bound for the client request itself.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - model_exists (bool): Whether the model was found\n",
    "            - deployment_status (str): Status of the model deployment\n",
    "            - inference_created (bool): Whether the endpoint creation was requested\n",
    "            - inference_status (str): Status message for inference endpoint\n",
    "    \"\"\"\n",
    "    model_result = deploy_ml_model(\n",
    "        es_client=es_client,\n",
    "        model_id=\"i-dot-ai__all-minilm-l6-v2-ukpga-6k-finetune\",\n",
    "        priority=\"normal\",\n",
    "    )\n",
    "\n",
    "    endpoint_result = create_inference_endpoint(\n",
    "        es_client=es_client,\n",
    "        model_id=model_id,\n",
    "        inference_id=inference_id,\n",
    "        request_timeout=inference_timeout,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"model_exists\": model_result[\"model_exists\"],\n",
    "        \"deployment_status\": model_result[\"deployment_status\"],\n",
    "        \"inference_created\": endpoint_result[\"inference_created\"],\n",
    "        \"inference_status\": endpoint_result[\"inference_status\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the UK legislation fine-tuned model and inference endpoint\n",
    "result = setup_inference_endpoint(\n",
    "    es_client=es_client,\n",
    "    model_id=model_id,\n",
    "    inference_id=inference_id,\n",
    "    priority=\"normal\",\n",
    "    inference_timeout=600,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Setup Result ---\")\n",
    "print(f\"Model exists: {result['model_exists']}\")\n",
    "print(f\"Deployment status: {result['deployment_status']}\")\n",
    "print(f\"Inference created: {result['inference_created']}\")\n",
    "print(f\"Inference status: {result['inference_status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline and index if they don't exist\n",
    "create_ingest_pipeline(\n",
    "    es_client=es_client,\n",
    "    pipeline_file=es_ingest_pipeline_file,\n",
    "    pipeline_name=es_ingest_pipeline_name,\n",
    ")\n",
    "\n",
    "create_index(\n",
    "    es_client=es_client, index_file=es_index_settings_file, index_name=es_index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to Elasticsearch\n",
    "load_data_to_elasticsearch(\n",
    "    es_client=es_client,\n",
    "    es_index_name=es_index_name,\n",
    "    data_dir=data_dir,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
