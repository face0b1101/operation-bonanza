{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Setting up Elasticsearch\n",
    "\n",
    "The process we'll follow is:\n",
    "\n",
    "1. Create an Elasticsearch client\n",
    "2. Check if the required ingest pipeline exists, and create it if not\n",
    "3. Check if the index with proper mappings exists, and create it if not\n",
    "4. Load the Reddit data into the index\n",
    "\n",
    "This ensures we don't overwrite existing settings if the index is already properly configured - and, importantly, don't ingest duplicate data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from collections.abc import Iterator\n",
    "\n",
    "from decouple import config\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Load environment variables from .env file\n",
    "ES_CLOUD_ID = config(\"ES_CLOUD_ID\", default=\"\")\n",
    "ES_API_KEY = config(\"ES_API_KEY\", default=\"\")\n",
    "\n",
    "# define Elasticsearch config files\n",
    "es_index_name = \"demo-chatroom.data-reddit\"\n",
    "es_index_settings_file = \"../elasticsearch/indices/demo-chatroom.data-reddit.json\"\n",
    "\n",
    "es_ingest_pipeline_name = \"reddit-chat-data-pipeline\"\n",
    "es_ingest_pipeline_file = \"../elasticsearch/pipelines/reddit-chat-data-pipeline.json\"\n",
    "\n",
    "# define data directory\n",
    "data_dir = \"../data/reddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Elasticsearch client\n",
    "if not ES_CLOUD_ID or not ES_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"ES_CLOUD_ID and ES_API_KEY must be set in the environment variables.\"\n",
    "    )\n",
    "\n",
    "es_client = Elasticsearch(\n",
    "    cloud_id=ES_CLOUD_ID,\n",
    "    api_key=ES_API_KEY,\n",
    ")\n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ingest_pipeline(\n",
    "    es_client: Elasticsearch, pipeline_file: str, pipeline_name: str\n",
    ") -> bool:\n",
    "    \"\"\"Create the ingest pipeline if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        es_client: Elasticsearch client\n",
    "        pipeline_file: Path to the pipeline definition file\n",
    "        pipeline_name: Name of the pipeline to create\n",
    "\n",
    "    Returns:\n",
    "        bool: True if pipeline was created, False if it already existed\n",
    "    \"\"\"\n",
    "    # Check if pipeline exists\n",
    "    try:\n",
    "        es_client.ingest.get_pipeline(id=pipeline_name)\n",
    "        print(f\"Pipeline '{pipeline_name}' already exists\")\n",
    "        return False\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Pipeline '{pipeline_name}' not found, creating it...\")\n",
    "\n",
    "        # Load pipeline definition from file\n",
    "        with open(pipeline_file) as file:\n",
    "            pipeline_definition = json.load(file)\n",
    "\n",
    "        # Create the pipeline\n",
    "        es_client.ingest.put_pipeline(id=pipeline_name, body=pipeline_definition)\n",
    "\n",
    "        print(f\"Pipeline '{pipeline_name}' created successfully\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(es_client: Elasticsearch, index_file: str, index_name: str):\n",
    "    \"\"\"Create the index if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        es_client: Elasticsearch client\n",
    "        index_file: Path to the index definition file\n",
    "        index_name: Name of the index to create\n",
    "\n",
    "    Returns:\n",
    "        bool: True if index was created, False if it already existed\n",
    "    \"\"\"\n",
    "    # Check if index exists and store the result\n",
    "    if es_client.indices.exists(index=index_name).body:\n",
    "        print(f\"Index '{index_name}' already exists\")\n",
    "    else:\n",
    "        # If index does not exist, create it\n",
    "        print(f\"Index '{index_name}' not found, creating it with proper mappings...\")\n",
    "\n",
    "        # Load index definition from file\n",
    "        with open(index_file) as file:\n",
    "            index_definition = json.load(file)\n",
    "\n",
    "        # Create the index with settings and mappings\n",
    "        if es_client.indices.create(index=index_name, body=index_definition).body:\n",
    "            print(f\"Index '{index_name}' created successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to create index '{index_name}'\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_ndjson_generator(filepath: str) -> Iterator[dict]:\n",
    "    \"\"\"Yield one JSON object at a time from an NDJSON file.\"\"\"\n",
    "    with open(filepath, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed JSON in {filepath}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reddit_data_to_elasticsearch(\n",
    "    es_client: Elasticsearch,\n",
    "    es_index_name: str,\n",
    "    data_dir: str = \"../data/reddit\",\n",
    "    excluded_subreddits: list | None = None,\n",
    "    retry_count: int = 3,\n",
    "    timeout: int = 30,\n",
    "):\n",
    "    \"\"\"Read Reddit NDJSON files, construct documents, and load to Elasticsearch using create (no duplicates).\n",
    "\n",
    "    Args:\n",
    "        es_client: Elasticsearch client\n",
    "        es_index_name: The index name to load data into\n",
    "        data_dir: Directory containing Reddit json files\n",
    "        excluded_subreddits: optional list of subreddits to exclude from processing\n",
    "        retry_count: Number of times to retry on timeout\n",
    "        timeout: Timeout in seconds for operations\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(f\"{data_dir}/*.ndjson\")\n",
    "    excluded_subreddits = excluded_subreddits or []\n",
    "\n",
    "    files_to_process = [\n",
    "        file\n",
    "        for file in all_files\n",
    "        if not any(\n",
    "            excluded in os.path.basename(file) for excluded in excluded_subreddits\n",
    "        )\n",
    "    ]\n",
    "    print(f\"Found {len(files_to_process)} files to process\")\n",
    "\n",
    "    total_docs = 0\n",
    "\n",
    "    for filepath in files_to_process:\n",
    "        subreddit = os.path.basename(filepath).split(\"-\")[2]\n",
    "        print(f\"Processing {subreddit}...\")\n",
    "\n",
    "        doc_count = 0\n",
    "\n",
    "        with open(filepath, encoding=\"utf-8\") as file:\n",
    "            total_lines = sum(1 for _ in file)\n",
    "\n",
    "        for data in tqdm(\n",
    "            reddit_ndjson_generator(filepath),\n",
    "            desc=f\"Loading {subreddit}\",\n",
    "            total=total_lines,\n",
    "            unit=\"doc\",\n",
    "        ):\n",
    "            try:\n",
    "                doc_id = str(uuid.uuid4())\n",
    "                success = False\n",
    "                attempts = 0\n",
    "                while not success and attempts < retry_count:\n",
    "                    try:\n",
    "                        es_client.create(\n",
    "                            index=es_index_name,\n",
    "                            id=doc_id,\n",
    "                            document=data,\n",
    "                            timeout=f\"{timeout}s\",\n",
    "                        )\n",
    "                        success = True\n",
    "                        doc_count += 1\n",
    "                    except Exception as e:\n",
    "                        if hasattr(e, \"status_code\") and e.status_code == 409:\n",
    "                            # Document already exists, skip\n",
    "                            success = True\n",
    "                        else:\n",
    "                            attempts += 1\n",
    "                            print(f\"Attempt {attempts}/{retry_count} failed: {e!s}\")\n",
    "                            if attempts >= retry_count:\n",
    "                                print(\n",
    "                                    f\"Failed to upload document after {retry_count} attempts\"\n",
    "                                )\n",
    "                                raise\n",
    "                            time.sleep(2)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed JSON in {filepath}\")\n",
    "                continue\n",
    "        total_docs += doc_count\n",
    "        print(f\"Loaded {doc_count} documents from {subreddit}\")\n",
    "    print(f\"Total documents loaded: {total_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline and index if they don't exist\n",
    "create_ingest_pipeline(\n",
    "    es_client=es_client,\n",
    "    pipeline_file=es_ingest_pipeline_file,\n",
    "    pipeline_name=es_ingest_pipeline_name,\n",
    ")\n",
    "\n",
    "create_index(\n",
    "    es_client=es_client, index_file=es_index_settings_file, index_name=es_index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_subreddits = [\n",
    "    # \"mildlyinteresting\",\n",
    "    # \"personalfinance\",\n",
    "    # \"philosophy\",\n",
    "    # \"podcasts\",\n",
    "    # \"programming\",\n",
    "    # \"relationship_advice\",\n",
    "    # \"science\",\n",
    "    # \"scifi\",\n",
    "    # \"Showerthoughts\",\n",
    "    # \"SkincareAddiction\",\n",
    "    # \"socialskills\",\n",
    "    # \"space\",\n",
    "    # \"sports\",\n",
    "    # \"suggestmeabook\",\n",
    "    # \"technology\",\n",
    "    # \"tifu\",\n",
    "    # \"todayilearned\",\n",
    "    # \"travel\",\n",
    "    # \"UpliftingNews\",\n",
    "    # \"WritingPrompts\",\n",
    "    # \"YouShouldKnow\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to load Reddit data to Elasticsearch\n",
    "load_reddit_data_to_elasticsearch(\n",
    "    es_client=es_client,\n",
    "    es_index_name=es_index_name,\n",
    "    data_dir=data_dir,\n",
    "    excluded_subreddits=excluded_subreddits,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
